import os
from langchain.schema import Document
import streamlit as st
import fitz
import pinecone
from pinecone import Pinecone
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote.korean import stopwords
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_teddynote.community.pinecone import(
    create_sparse_encoder,
    preprocess_documents,
    create_index,
    init_pinecone_index,
    fit_sparse_encoder
)
from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever
from langchain_teddynote.community.pinecone import upsert_documents
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain.vectorstores import Pinecone as LangchainPinecone
from langserve import RemoteRunnable
from langchain_upstage import UpstageEmbeddings
from dotenv import load_dotenv


# API í‚¤ ë¡œë“œ
load_dotenv()

# Pinecone ì„¤ì •
PINECONE_API_KEY = os.environ["PINECONE_API_KEY"]
PINECONE_ENV = os.environ["PINECONE_ENV"]

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
LANGSERVE_ENDPOINT = "http://localhost:8001/chat/c/N4XyA/"

UPSTAGE_API_KEY= os.environ["UPSTAGE_API_KEY"]

# Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
embedding = UpstageEmbeddings(
    model="solar-embedding-1-large-passage",
    upstage_api_key=os.getenv("UPSTAGE_API_KEY")
)

host = os.getenv("PINECONE_HOST")

pc = Pinecone(api_key=PINECONE_API_KEY, embedding=embedding)

# ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì •
INDEX_NAME = "hihello-db-index"

# Pinecone Index ê°ì²´ë¡œ ì—°ê²°
index = pinecone.Index(index_name = INDEX_NAME, api_key = PINECONE_API_KEY, host=host)
print(f"Index '{INDEX_NAME}' is ready to use.")

RAG_PROMPT_TEMPLATE = """

You are hihelloAI, a helpful assistant. Respond concisely and accurately to the user's question in 2 lines or less.

- If the user's input explicitly matches greetings like "ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ã…ã…‡", "í•˜ì´", "í—¬ë¡œìš°", "hi", or similar, respond with:
  "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š."

- For questions about introducing yourself like "ë„ˆì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜", "ë„ˆì— ëŒ€í•´ ë§í•´ë´", "ë„Œ ëˆ„êµ¬ì•¼", "ë„ˆì— ëŒ€í•´ ë§í•´", "ë„ˆì— ëŒ€í•´ ë§í•´ì¤˜", "ë„ˆì— ëŒ€í•´ ì„¤ëª…í•´", "ë„ˆì— ëŒ€í•´ ì„¤ëª…í•´ë´", or similar, respond with:
  "ì €ëŠ” hihelloAIì…ë‹ˆë‹¤. Ollama, Llama3, Pinecone ìœ¼ë¡œ êµ¬í˜„ëœ ìì²´ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ RAG ì‹œìŠ¤í…œì„ ì´ìš©í•˜ì—¬ ì»¤ìŠ¤í„°ë§ˆì´ì§• ëœ ë‹µë³€ì„ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €ì˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì—¬ëŸ¬ë¶„ì˜ ì¸í„´ ìƒí™œì„ ìµœëŒ€í•œ ë„ì™€ë“œë¦´ê²Œìš”!"

- If the user's question is unrelated to the retrieved context or if the answer is not found in the context, respond with:
  "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!"

- If you don't know the answer, respond with:
  "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!"

Do not include any additional context, references, or links in your answer unless explicitly asked by the user. 

Question: {question}
Context: {context}
Answer:
"""

st.set_page_config(page_title="HiHello ChatBot", page_icon="ğŸ’¬")
st.title("HiHello ChatBot")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë°˜ê°€ì›Œìš” HiHello AI ì…ë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Embedding file...")
def embed_file(uploaded_file):
    # íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ ì²˜ë¦¬
    file_content = uploaded_file.read()

    # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬ (ì˜ˆ: PDF, TXT ë“±)
    if uploaded_file.type == "application/pdf":
        # PDF íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬
        doc = fitz.open(stream=file_content, filetype="pdf")
        split_docs = []
        for page in doc:
            text = page.get_text()
            split_docs.append(text)
    else:
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
        split_docs = file_content.decode("utf-8").splitlines()

    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )

    # í…ìŠ¤íŠ¸ ë¶„í•  (Document ê°ì²´ë¡œ ë³€í™˜)
    split_docs = [Document(page_content=text) for text in split_docs]
    split_docs = text_splitter.split_documents(split_docs)

    # íŒŒì¼ì˜ ì´ë¦„ì„ ë©”íƒ€ë°ì´í„°ë¡œ ì„¤ì •
    filename = uploaded_file.name
    for doc in split_docs:
        doc.metadata = {"source": filename}  # ë©”íƒ€ë°ì´í„°ë¥¼ ê° Document ê°ì²´ì— ì¶”ê°€

    # ì „ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
    contents, metadatas = preprocess_documents(
        split_docs=split_docs,
        metadata_keys=["source"],
        min_length=5,
        use_basename=True,  # ì´ ê°’ì´ Trueì¼ ê²½ìš° basename ì‚¬ìš©
    )

    # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    pc = Pinecone(api_key=PINECONE_API_KEY, embedding=embedding)
    index = pinecone.Index(index_name = INDEX_NAME, api_key = PINECONE_API_KEY, host=host)
    # Pinecone Index ê°ì²´ë¡œ ì—°ê²°
    index = pinecone.Index(index_name=INDEX_NAME, api_key=PINECONE_API_KEY, host=host)
    print(f"Index '{INDEX_NAME}' is ready to use.")

    # Pinecone ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸í•˜ê¸°
    if INDEX_NAME not in pc.list_indexes():
        print("ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        pc_index = create_index(
            api_key=os.environ["PINECONE_API_KEY"],
            index_name=INDEX_NAME,
            dimension=4096,  # ì„ë² ë”© ì°¨ì›
            metric="dotproduct",
        )
    else:
        pc_index = index(INDEX_NAME)  # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©

    # Sparse encoder ìƒì„±
    sparse_encoder = create_sparse_encoder(stopwords(), mode="kiwi")

    # Sparse encoder í›ˆë ¨ ë° ì €ì¥
    saved_path = fit_sparse_encoder(
        sparse_encoder=sparse_encoder, contents=contents, save_path="./sparse_encoder.pkl"
    )
    print("ì„ë² ë”©ì„ ì‹œì‘")
    # Pineconeì— ë²¡í„° ì—…ë¡œë“œ
    upsert_documents(
        index=pc_index,  # Pinecone ì¸ë±ìŠ¤
        namespace="hihello-namespace-01",  # Pinecone namespace
        contents=contents,  # ì „ì²˜ë¦¬ëœ ë¬¸ì„œ ë‚´ìš©
        metadatas=metadatas,  # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        sparse_encoder=sparse_encoder,  # Sparse encoder
        embedder=embedding,  # ì„ë² ë”© ê°ì²´
        batch_size=32,
    )

    print("ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸")
    # Pinecone ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
    pc_index.describe_index_stats()


# # Streamlit UIë¥¼ ìœ„í•œ íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
# uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["txt", "pdf", "docx", "csv"])
#
# if uploaded_file is not None:
#     retriever = embed_file(uploaded_file)
#     st.write("íŒŒì¼ ì„ë² ë”© ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)

    with st.chat_message("assistant"):
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT)
        with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...."):

            chat_container = st.empty()

            user_vector = embedding.embed_query(user_input)

            # ê²€ìƒ‰ìš© ë¦¬íŠ¸ë¦¬ë²„(PineconeKiwiHybridRertriever ì´ˆê¸°í™”)
            pinecone_params = init_pinecone_index(
                index_name="hihello-db-index",  # Pinecone ì¸ë±ìŠ¤ ì´ë¦„
                namespace="hihello-namespace-01",  # Pinecone Namespace
                api_key=os.environ["PINECONE_API_KEY"],  # Pinecone API Key
                sparse_encoder_path="./sparse_encoder.pkl",  # Sparse Encoder ì €ì¥ê²½ë¡œ(save_path)
                stopwords=stopwords(),  # ë¶ˆìš©ì–´ ì‚¬ì „
                tokenizer="kiwi",
                embeddings=UpstageEmbeddings(
                    model="solar-embedding-1-large-query"
                ),  # Dense Embedder
                top_k=5,  # Top-K ë¬¸ì„œ ë°˜í™˜ ê°œìˆ˜
                alpha=0.75,  # alpha=0.75ë¡œ ì„¤ì •í•œ ê²½ìš°, (0.75: Dense Embedding, 0.25: Sparse Embedding)
            )

            # íŒŒì¸ì½˜ ê²€ìƒ‰ê¸° ìƒì„±
            pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)

            # ì‹¤í–‰ ê²°ê³¼
            print(user_input)
            search_results = pinecone_retriever.invoke(user_input, search_kwargs={"alpha": 1, "k": 3})

            if search_results is None or all(result.metadata is None for result in search_results):
                # DBì—ì„œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ ì²˜ë¦¬
                if user_input.lower() in ["ì•ˆë…•", "ë„Œ ëˆ„êµ¬ì•¼", "ì–´ë–»ê²Œ ì§€ë‚´", "hi"]:
                    response = "ì•ˆë…•í•˜ì„¸ìš”! ì—¬ëŸ¬ë¶„ì˜ ì¸í„´ìƒí™œì— ë„ì›€ì„ ë“œë¦´ HiHello AIì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š"
                    chat_container.markdown(response)
                    add_history("ai", response)
                else:
                    # DB ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ëŒ€í™”í˜• ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
                    response = "DBì—ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ê·¸ë ‡ì§€ë§Œ ì €ëŠ” í•­ìƒ ëŒ€í™”í•  ì¤€ë¹„ê°€ ë˜ì–´ ìˆì–´ìš”!"
                    chat_container.markdown(response)
                    add_history("ai", response)
            else:
                # DBì—ì„œ ê´€ë ¨ ê²°ê³¼ê°€ ìˆì„ ê²½ìš°
                for result in search_results:
                    print(result.page_content)
                    print(result.metadata)
                    print("\n====================\n")

                # íŒŒì¸ì½˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œí•œ ì±—ë´‡ ì¶œë ¥
                prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
                context = format_docs(search_results)
                # formatted_docsë¥¼ ë¬¸ìì—´ì—ì„œ Runnableë¡œ ë³€í™˜
                rag_chain = (
                        {
                            "context": RunnablePassthrough() | (lambda _: context),
                            "question": RunnablePassthrough(),
                        }
                        | prompt
                        | ollama
                        | StrOutputParser()
                )
                answer = rag_chain.stream(user_input)
                chunks = []
                for chunk in answer:
                    chunks.append(chunk)
                    chat_container.markdown("".join(chunks))
                add_history("ai", "".join(chunks))

