import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_teddynote.korean import stopwords
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores.faiss import FAISS
from langchain_teddynote.community.pinecone import create_index
from langserve import RemoteRunnable
from dotenv import load_dotenv

import glob
from langchain_teddynote.community.pinecone import(
    create_sparse_encoder,
    fit_sparse_encoder,
    preprocess_documents
)
from langchain_teddynote import logging

# â­ï¸ Embedding ì„¤ì •
USE_BGE_EMBEDDING = True

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
LANGSERVE_ENDPOINT = "http://localhost:8000/chat/c/N4XyA"

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± @Mineru
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# í”„ë¡¬í”„íŠ¸ë¥¼ ììœ ë¡­ê²Œ ìˆ˜ì •í•´ ë³´ì„¸ìš”!
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” hihelloAI ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
Question: {question} 
Context: {context} 
Answer:"""

st.set_page_config(page_title="OLLAMA Local ëª¨ë¸ í…ŒìŠ¤íŠ¸", page_icon="ğŸ’¬")
st.title("OLLAMA Local ëª¨ë¸ í…ŒìŠ¤íŠ¸")

# API í‚¤ ë¡œë“œ
load_dotenv()
# ë¶ˆìš©ì–´ ì²˜ë¦¬
stopwords()[:20]

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)

split_docs = []

# í…ìŠ¤íŠ¸ íŒŒì¼ Load -> List[Document] í˜•íƒœë¡œ ë³€í™˜
files = sorted(glob.glob("./data/*.pdf"))
print("files:", files)

for file in files:
    loader = PyMuPDFLoader(file)
    split_docs.extend(loader.load_and_split(text_splitter))

# ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
print(f"ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ {len(split_docs)}")
print(f"PDF íŒŒì¼ ê²½ë¡œ: {files}")
if not files:
    print("No PDF files found in the data folder!")
# ì²«ë²ˆì§¸ data ì œëª© ë¡œê¹…
print(f"í˜ì´ì§€ ë¡œê¹… {split_docs[0].page_content}")

# ì²«ë²ˆì§¸ data ë©”íƒ€ë°ì´í„° ë¡œê¹…
print(f"ë©”íƒ€ë°ì´í„° ë¡œê¹… {split_docs[0].metadata}")

contents, metadatas = preprocess_documents(
    split_docs=split_docs,
    metadata_keys=["source", "page", "author"],
    min_length=5,
    use_basename=True,
)

# VectorStore ì— ì €ì¥í•  ë¬¸ì„œ í™•ì¸
print(f"VectorStore ì— ì €ì¥í•  ë¬¸ì„œ í™•ì¸ {contents[:5]}")

# VectorStore ì— ì €ì¥í•  metadata í™•ì¸
print(f"VectorStore ì— ì €ì¥í•  metadata í™•ì¸ {metadatas.keys()}")

# metadata ì—ì„œ source ë¥¼ í™•ì¸
print(metadatas["source"][:5])

# ë¬¸ì„œ ê°œìˆ˜ í™•ì¸, ì†ŒìŠ¤ ê°œìˆ˜ í™•ì¸, í˜ì´ì§€ ê°œìˆ˜ í™•ì¸
len(contents), len(metadatas["source"]), len(metadatas["page"])
print(len(contents), len(metadatas["source"]), len(metadatas["page"]))

# íŒŒì¸ì½˜ ì¸ë±ìŠ¤ ìƒì„±
pc_index = create_index(
    api_key=os.environ["PINECONE_API_KEY"],
    index_name="hihello-db-index",
    dimension=4096, # ì„ë² ë”©ì€ UpstageEmbeddings ì¨ì„œ 4096 ì°¨ì›ìœ¼ë¡œ í–ˆìŒ
    metric="dotproduct", #í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ì•Œê³ ë¦¬ì¦˜ ì“°ê²Œ ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ Dotproductë¡œ ì§€ì •
)

sparse_encoder = create_sparse_encoder(stopwords(), mode="kiwi")

saved_path = fit_sparse_encoder(
    sparse_encoder=sparse_encoder, contents=contents, save_path="./sparse_encoder.pkl"
)


from langchain_upstage import UpstageEmbeddings

upstage_embeddings = UpstageEmbeddings(model="solar-embedding-1-large-passage", upstage_api_key=os.getenv("UPSTAGE_API_KEY"))

from langchain_teddynote.community.pinecone import upsert_documents

upsert_documents(
    index=pc_index,  # Pinecone ì¸ë±ìŠ¤
    namespace="hihello-namespace-01",  # Pinecone namespace
    contents=contents,  # ì´ì „ì— ì „ì²˜ë¦¬í•œ ë¬¸ì„œ ë‚´ìš©
    metadatas=metadatas,  # ì´ì „ì— ì „ì²˜ë¦¬í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
    sparse_encoder=sparse_encoder,  # Sparse encoder
    embedder=upstage_embeddings,
    batch_size=32,
)

pc_index.describe_index_stats()

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]


def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)

# ì„ë² ë”©
@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=text_splitter)

    if USE_BGE_EMBEDDING:
        # BGE Embedding: @Mineru
        model_name = "BAAI/bge-m3"
        # GPU Device ì„¤ì •:
        # - NVidia GPU: "cuda"
        # - Mac M1, M2, M3: "mps"
        # - CPU: "cpu"
        model_kwargs = {
            # "device": "cuda"
            "device": "mps"
            # "device": "cpu"
        }
        encode_kwargs = {"normalize_embeddings": True}
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
        )

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)


with st.sidebar:
    file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever = embed_file(file)

print_history()


if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        #ì£¼ì†Œ ì„¤ì •
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT)

        chat_container = st.empty()
        if file is not None:
            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | ollama
                | StrOutputParser()
            )
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            answer = rag_chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))
        else:
            prompt = ChatPromptTemplate.from_template(
                "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
            )

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            chain = prompt | ollama | StrOutputParser()

            answer = chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))