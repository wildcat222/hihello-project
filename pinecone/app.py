import os
import streamlit as st
import pinecone
from pinecone import Pinecone
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_teddynote.korean import stopwords
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_teddynote.community.pinecone import(
    create_sparse_encoder,
    preprocess_documents,
    create_index,
    init_pinecone_index,
    fit_sparse_encoder
)
from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever
from langchain_teddynote.community.pinecone import upsert_documents
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain.vectorstores import Pinecone as LangchainPinecone
from langserve import RemoteRunnable
from langchain_upstage import UpstageEmbeddings
from dotenv import load_dotenv

# API í‚¤ ë¡œë“œ
load_dotenv()

# Pinecone ì„¤ì •
PINECONE_API_KEY = os.environ["PINECONE_API_KEY"]
PINECONE_ENV = os.environ["PINECONE_ENV"]

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
LANGSERVE_ENDPOINT = "http://localhost:8000/chat/c/N4XyA"

UPSTAGE_API_KEY= os.environ["UPSTAGE_API_KEY"]

# Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
embedding = UpstageEmbeddings(
    model="solar-embedding-1-large-passage",
    upstage_api_key=os.getenv("UPSTAGE_API_KEY")
)

host = os.getenv("PINECONE_HOST")

pc = Pinecone(api_key=PINECONE_API_KEY, embedding=embedding)

# ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì •
INDEX_NAME = "hihello-db-index"

# Pinecone Index ê°ì²´ë¡œ ì—°ê²°
index = pinecone.Index(index_name = INDEX_NAME, api_key = PINECONE_API_KEY, host=host)
print(f"Index '{INDEX_NAME}' is ready to use.")

# í”„ë¡¬í”„íŠ¸ë¥¼ ììœ ë¡­ê²Œ ìˆ˜ì •í•´ ë³´ì„¸ìš”!
RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” hihelloAI ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
Question: {question} 
Context: {context} 
Answer:"""

st.set_page_config(page_title="HiHello ChatBot", page_icon="ğŸ’¬")
st.title("HiHello ChatBot")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë°˜ê°€ì›Œìš” HiHello AI ì…ë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    split_docs = []
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )
    loader = UnstructuredFileLoader(file_path)
    split_docs.extend(loader.load_and_split(text_splitter=text_splitter))

    contents, metadatas = preprocess_documents(
        split_docs=split_docs,
        metadata_keys=["source"],
        min_length=5,
        use_basename=True,
    )

    # íŒŒì¸ì½˜ ì¸ë±ìŠ¤ ìƒì„±
    if INDEX_NAME not in pc.list_indexes().names():
        print("ì¸ë±ìŠ¤ 2ì°¨")
        pc_index = create_index(
            api_key=os.environ["PINECONE_API_KEY"],
            index_name="hihello-db-index",
            dimension=4096,  # ì„ë² ë”©ì€ UpstageEmbeddings ì¨ì„œ 4096 ì°¨ì›ìœ¼ë¡œ í–ˆìŒ
            metric="dotproduct",  # í•˜ì´ë¸Œë¦¬ë“œ ì„œì¹˜ ì•Œê³ ë¦¬ì¦˜ ì“°ê²Œ ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ Dotproductë¡œ ì§€ì •
        )

    sparse_encoder = create_sparse_encoder(stopwords(), mode="kiwi")

    saved_path = fit_sparse_encoder(
        sparse_encoder=sparse_encoder, contents=contents, save_path="./sparse_encoder.pkl"
    )

    upsert_documents(
        index=index,  # Pinecone ì¸ë±ìŠ¤
        namespace="hihello-namespace-01",  # Pinecone namespace
        contents=contents,  # ì´ì „ì— ì „ì²˜ë¦¬í•œ ë¬¸ì„œ ë‚´ìš©
        metadatas=metadatas,  # ì´ì „ì— ì „ì²˜ë¦¬í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        sparse_encoder=sparse_encoder,  # Sparse encoder
        embedder=embedding,
        batch_size=32,
    )

    index.describe_index_stats()

    # LangchainPineconeìœ¼ë¡œ ë¬¸ì„œë¥¼ Pineconeì— ì—…ë¡œë“œ
    # LangchainPinecone.from_documents(docs, embedding=embedding, index_name=INDEX_NAME)

    retriever = LangchainPinecone(index=index, text_key= "content", embedding=embedding).as_retriever()
    return retriever


with st.sidebar:
    file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever = embed_file(file)

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT)
        with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...."):

            chat_container = st.empty()

            user_vector = embedding.embed_query(user_input)

            # ê²€ìƒ‰ìš© ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            pinecone_params = init_pinecone_index(
                index_name="hihello-db-index",  # Pinecone ì¸ë±ìŠ¤ ì´ë¦„
                namespace="hihello-namespace-01",  # Pinecone Namespace
                api_key=os.environ["PINECONE_API_KEY"],  # Pinecone API Key
                sparse_encoder_path="./sparse_encoder.pkl",  # Sparse Encoder ì €ì¥ê²½ë¡œ(save_path)
                stopwords=stopwords(),  # ë¶ˆìš©ì–´ ì‚¬ì „
                tokenizer="kiwi",
                embeddings=UpstageEmbeddings(
                    model="solar-embedding-1-large-query"
                ),  # Dense Embedder
                top_k=1,  # Top-K ë¬¸ì„œ ë°˜í™˜ ê°œìˆ˜
                alpha=0.5,  # alpha=0.75ë¡œ ì„¤ì •í•œ ê²½ìš°, (0.75: Dense Embedding, 0.25: Sparse Embedding)
            )

            # íŒŒì¸ì½˜ ê²€ìƒ‰ê¸° ìƒì„±
            pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)

            # ì‹¤í–‰ ê²°ê³¼
            print(user_input)
            search_results = pinecone_retriever.invoke(user_input, search_kwargs={"alpha": 1, "k" : 1})
            for result in search_results:
                print(result.page_content)
                print(result.metadata)
                print("\n====================\n")

            # íŒŒì¸ì½˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œí•œ ì±—ë´‡ ì¶œë ¥
            if search_results is not None:
                prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)
                context = format_docs(search_results)
                # formatted_docsë¥¼ ë¬¸ìì—´ì—ì„œ Runnableë¡œ ë³€í™˜
                rag_chain = (
                        {
                            "context": RunnablePassthrough() | (lambda _: context),
                            "question": RunnablePassthrough(),
                        }
                        | prompt
                        | ollama
                        | StrOutputParser()
                )
                answer = rag_chain.stream(user_input)
                chunks = []
                for chunk in answer:
                    chunks.append(chunk)
                    chat_container.markdown("".join(chunks))
                add_history("ai", "".join(chunks))
            else:
                print("else")
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ëŒ€ë‹µ
                prompt = ChatPromptTemplate.from_template(
                    "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
                )
                chain = prompt | ollama | StrOutputParser()
                answer = chain.stream(user_input)
                print(3)
                chunks = []
                print(41)
                for chunk in answer:
                    print(5)
                    chunks.append(chunk)
                    print(6)
                    chat_container.markdown("".join(chunks))
                print(7)
                add_history("ai", "".join(chunks))
                print("finished")